# **Transformers**

Natural Language Processing, NLP துறையில் **Transformers** என்பது ஒரு புரட்சிகர மாற்றத்தை ஏற்படுத்தியுள்ளது. இது 2017-ல் Google-ஆல் அறிமுகப்படுத்தப்பட்ட "Attention is All You Need" என்ற ஆராய்ச்சிக் கட்டுரையில் முன்மொழியப்பட்ட ஒரு மேம்பட்ட நரவலை (Neural Networks) கட்டமைப்பாகும். Transformers-ன் முக்கிய புதுமை என்னவென்றால், இது முன்பு பயன்படுத்தப்பட்ட RNN (Recurrent Neural Networks) மற்றும் LSTM (Long Short-Term Memory) போன்ற வரிசை-சார்ந்த மாதிரிகளை விட, **Self-Attention Mechanism**-ஐ அடிப்படையாகக் கொண்டது. இந்த மெக்கானிசம், உரையில் உள்ள சொற்களுக்கிடையேயான சார்புகளை மிகத் துல்லியமாக புரிந்துகொள்ள உதவுகிறது, மேலும் இது Parallel Processing-யை மேம்படுத்துவதன் மூலம் கணிப்புகளை வேகமாகவும் திறம்படவும் செய்கிறது.

Transformers-ன் அடிப்படையில் BERT (Bidirectional Encoder Representations from Transformers), GPT (Generative Pre-trained Transformer), T5 (Text-To-Text Transfer Transformer) போன்ற மாதிரிகள் உருவாக்கப்பட்டு, NLP துறையில் பல முன்னேற்றங்களை ஏற்படுத்தியுள்ளன. இந்த மாதிரிகள் பல மொழி-சார்ந்த பணிகளில் (Language Tasks) சிறந்த செயல்திறனை வெளிப்படுத்துகின்றன, எடுத்துக்காட்டாக, மொழிபெயர்ப்பு (Translation), உரை சுருக்கம் (Text Summarization), வினா-விடை அமைப்புகள் (Question Answering), உணர்வு பகுப்பாய்வு (Sentiment Analysis) மற்றும் உரை உருவாக்கம் (Text Generation) போன்றவை. Transformers-ன் நெகிழ்வுத்தன்மை மற்றும் பல்துறைத்தன்மை காரணமாக, இது Computer Vision மற்றும் பிற AI துறைகளிலும் பயன்படுத்தப்படுகிறது.

இந்த கட்டுரையில், Transformers-ன் கட்டமைப்பு, அதன் பண்புகள், மற்றும் NLP-ல் அதன் பயன்பாடுகள் பற்றி விரிவாக பார்ப்போம். மேலும், இது எவ்வாறு NLP துறையில் புதிய தரநிலைகளை நிர்ணயித்து, AI-ன் எதிர்காலத்தை வடிவமைக்கிறது என்பதையும் ஆராய்வோம்.

## **Transformers-ன் கட்டமைப்பு **

Transformers என்பது **Encoder-Decoder** கட்டமைப்பை கொண்ட ஒரு நரவலை ஆகும். இது **Attention Mechanism**-ஐ மையமாக கொண்டு உருவாக்கப்பட்டது. இப்போது, Transformers-ன் ஒவ்வொரு பகுதியையும் விரிவாக பார்ப்போம்.

---

### **1. உள்ளீட்டு பிரதிநிதித்துவம் (Input Representation)**

Transformers மாதிரிகள் உரை தரவை (Text Data) செயல்படுத்துவதற்கு முன்பு, அதை ஒரு கணித வடிவத்தில் மாற்ற வேண்டும். இந்த செயல்முறையில், உரையில் உள்ள ஒவ்வொரு வார்த்தையும் ஒரு **Embedding** என்று அழைக்கப்படும் ஒரு எண் வெக்டராக (Vector) மாற்றப்படுகிறது. இந்த வெக்டர்கள் உரையின் அர்த்தத்தை பிரதிநிதித்துவப்படுத்தும் வகையில் வடிவமைக்கப்படுகின்றன.

#### **Embeddings:**

- ஒரு வார்த்தை ($w$) என்பது ஒரு embedding வெக்டர் $( \mathbf{e}_w \in \mathbb{R}^d)$ ஆக மாற்றப்படுகிறது. இங்கு $(d)$ என்பது embedding பரிமாணம் (Dimension). எடுத்துக்காட்டாக, ஒரு வார்த்தை "cat" என்பது ஒரு 512-பரிமாண வெக்டராக மாற்றப்படலாம்.
- Embeddings என்பது ஒரு வார்த்தையின் அர்த்தத்தை எண்களின் வடிவில் பிரதிநிதித்துவப்படுத்தும் ஒரு முறை. இது மாதிரிக்கு உரையை புரிந்துகொள்ள உதவுகிறது.

#### **Positional Encoding:**

Transformers மாதிரிகள் உரையில் உள்ள வார்த்தைகளின் வரிசை (Sequence) பற்றிய தகவலை பாதுகாக்க வேண்டும். ஆனால், Transformers மாதிரிகள் ஒரே நேரத்தில் முழு உரையையும் செயல்படுத்துவதால், வார்த்தைகளின் நிலை (Position) பற்றிய தகவலை சேமிக்க ஒரு சிறப்பு முறை பயன்படுத்தப்படுகிறது. இதற்கு **Positional Encoding** என்று பெயர்.

- Positional Encoding $$(\mathbf{P} \in \mathbb{R}^{n \times d})$$ என்பது ஒரு மேட்ரிக்ஸ் (Matrix) ஆகும், இது உரையில் உள்ள ஒவ்வொரு வார்த்தையின் நிலையை (Position) பிரதிநிதித்துவப்படுத்துகிறது.
- இது சைன் (Sine) மற்றும் கொசைன் (Cosine) செயல்பாடுகளை பயன்படுத்தி கணக்கிடப்படுகிறது. இந்த செயல்பாடுகள் வார்த்தைகளின் நிலையை ஒரு தனித்துவமான வடிவில் குறிக்கின்றன.

Positional Encoding கணக்கிடுவதற்கான சூத்திரங்கள்:

$$
[\mathbf{P}_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{\frac{2i}{d}}}\right)]
$$

----

$$
[\mathbf{P}_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{\frac{2i}{d}}}\right)]
$$

இங்கு:

- $( pos )$ என்பது வார்த்தையின் நிலை (Position). எடுத்துக்காட்டாக, "I love cats" என்ற வாக்கியத்தில், "I" என்பது position 1, "love" என்பது position 2, மற்றும் "cats" என்பது position 3.
- $( i )$ என்பது embedding பரிமாணத்தின் குறியீடு (Index). எடுத்துக்காட்டாக, embedding பரிமாணம் 512 எனில்,$( i )$ என்பது 0 முதல் 255 வரை இருக்கும்.
- $( d )$ என்பது embedding பரிமாணம் (Dimension).

#### **Embeddings + Positional Encoding:**

Embeddings மற்றும் Positional Encoding ஆகியவை ஒன்றாக சேர்க்கப்படுகின்றன. இதன் மூலம், மாதிரிக்கு ஒவ்வொரு வார்த்தையின் அர்த்தம் மற்றும் அதன் நிலை இரண்டும் தெரியும்.

$$
[\mathbf{P}_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{\frac{2i}{d}}}\right)]
$$


இங்கு:

- ( $$\mathbf{E}$$ ) என்பது embeddings மேட்ரிக்ஸ்.
- ( $$\mathbf{P}$$ ) என்பது positional encoding மேட்ரிக்ஸ்.
- ( $$\mathbf{X}$$ ) என்பது இறுதி உள்ளீட்டு பிரதிநிதித்துவம்.

இந்த உள்ளீட்டு பிரதிநிதித்துவம் Transformers மாதிரிக்கு உரையை புரிந்துகொள்ள உதவுகிறது. இது மாதிரிக்கு உரையில் உள்ள வார்த்தைகளின் அர்த்தம் மற்றும் அவற்றின் வரிசை பற்றிய தகவலை ஒரே நேரத்தில் வழங்குகிறது.

---

### **2. குறியாக்கி (Encoder)**

Encoder என்பது Transformers மாதிரியின் முதல் முக்கிய பகுதியாகும். இது உரை தரவை (Text Data) embeddings-ஆக மாற்றி, அதை பல layers (அடுக்குகள்) மூலம் செயலாக்குகிறது. ஒவ்வொரு Encoder layer-உம் இரண்டு முக்கிய பகுதிகளை கொண்டிருக்கிறது:

1. **Multi-Head Self-Attention**
2. **Feedforward Neural Network**

இந்த பகுதிகள் உரையில் உள்ள வார்த்தைகளுக்கிடையேயான உறவுகளை புரிந்துகொள்வதற்கும், அவற்றின் அர்த்தத்தை பிரதிநிதித்துவப்படுத்துவதற்கும் உதவுகின்றன.

---

#### **2.1 Multi-Head Self-Attention:**

Self-Attention என்பது Transformers மாதிரியின் மிக முக்கியமான கருத்தாகும். இது ஒரு வார்த்தையை அதே வாக்கியத்தில் உள்ள மற்ற வார்த்தைகளுடன் ஒப்பிட்டு, அதன் அர்த்தத்தை புரிந்து கொள்ள உதவுகிறது. எடுத்துக்காட்டாக, "The cat sat on the mat" என்ற வாக்கியத்தில், "cat" என்ற வார்த்தை "sat" மற்றும் "mat" போன்ற வார்த்தைகளுடன் எவ்வாறு தொடர்புடையது என்பதை Self-Attention மூலம் புரிந்துகொள்ள முடியும்.

**Self-Attention Mechanism:**

Self-Attention செயல்முறை பின்வரும் படிகளை கொண்டுள்ளது:

1. **Queries, Keys, and Values:**
   - ஒவ்வொரு வார்த்தையும் மூன்று வெக்டர்களை கொண்டிருக்கிறது:
     - **Query ($$\mathbf{Q}$$)**: இது ஒரு வார்த்தை "என்னை பற்றி என்ன தெரியும்?" என்று கேட்கும் ஒரு கேள்வி போன்றது.
     
     - **Key ( $$\mathbf{K}$$ )**: இது ஒரு வார்த்தை "நான் என்ன தகவலை கொண்டிருக்கிறேன்?" என்று கூறும் ஒரு பதில் போன்றது.
     
     - **Value ( $$\mathbf{V}$$ )**: இது ஒரு வார்த்தை "நான் என்ன தகவலை வழங்க முடியும்?" என்று கூறும் ஒரு பதில் போன்றது.
     
     - இந்த வெக்டர்கள் embeddings-ஐ linear transformations (நேரியல் மாற்றங்கள்) மூலம் பெறப்படுகின்றன:
       $$
       [
       \mathbf{Q} = \mathbf{X} \mathbf{W}_Q, \quad \mathbf{K} = \mathbf{X} \mathbf{W}_K, \quad \mathbf{V} = \mathbf{X} \mathbf{W}_V
       ]
       $$
       
     
       இங்கு $$( \mathbf{W}_Q, \mathbf{W}_K, \mathbf{W}_V)$$ என்பது trainable weights (பயிற்சி மூலம் கற்றுக்கொள்ளப்படும் எடைகள்).
   
2. **Attention Scores:**
   
   - Query மற்றும் Key-ஐ பயன்படுத்தி, attention scores கணக்கிடப்படுகிறது. இது ஒரு வார்த்தை மற்ற வார்த்தைகளுடன் எவ்வளவு தொடர்புடையது என்பதை கணக்கிடுகிறது:
     $$
     [
     \text{Attention Scores} = \frac{\mathbf{Q} \mathbf{K}^T}{\sqrt{d_k}}
     ]
     $$
     
     
   - இங்கு $( d_k )$ என்பது Key-ன் பரிமாணம் (Dimension). இந்த பிரிவு $(\sqrt{d_k})$ attention scores-ஐ நிலைப்படுத்த (Stabilize) உதவுகிறது.
   
3. **Softmax:**
   
   - Attention scores-ஐ softmax செயல்பாடு மூலம் நிகழ்தகவுகளாக (Probabilities) மாற்றுகிறது. இது ஒரு வார்த்தை மற்ற வார்த்தைகளுடன் எவ்வளவு கவனம் செலுத்த வேண்டும் என்பதை தீர்மானிக்கிறது:
     $$
     [
     \text{Attention Weights} = \text{Softmax}\left(\frac{\mathbf{Q} \mathbf{K}^T}{\sqrt{d_k}}\right)
     ]
     $$
     
   
4. **Weighted Sum:**
   - Attention weights-ஐ Value-உடன் பெருக்கி, weighted sum கணக்கிடப்படுகிறது. இது ஒரு வார்த்தையின் இறுதி பிரதிநிதித்துவத்தை கொடுக்கிறது:
     $$
     [
     \text{Attention Output} = \text{Attention Weights} \cdot \mathbf{V}
     ]
     $$
     

**Multi-Head Attention:**

- Self-Attention-ஐ பல தலைகள் (Heads) மூலம் செயல்படுத்துகிறது. இது மாதிரியை பல கோணங்களில் (Perspectives) உரையை புரிந்து கொள்ள உதவுகிறது. ஒவ்வொரு head-உம் தனி $(\mathbf{Q}, \mathbf{K}, \mathbf{V})$ வெக்டர்களை கொண்டிருக்கிறது.
- ஒவ்வொரு head-உம் வெவ்வேறு விதமாக உரையை புரிந்துகொள்வதால், இது மாதிரியின் செயல்திறனை மேம்படுத்துகிறது.
- இறுதியில், அனைத்து heads-ன் வெளியீடுகளும் concatenate (இணைக்கப்படுகின்றன) செய்யப்பட்டு, linear transformation மூலம் ஒருங்கிணைக்கப்படுகிறது.

---

#### **2.2 Feedforward Neural Network:**

Self-Attention-ன் வெளியீட்டை மேலும் செயலாக்க, ஒரு Feedforward Neural Network (FFN) பயன்படுத்தப்படுகிறது. இது ஒரு எளிய நரம்பியல் வலைப்பின்னல் (Neural Network) ஆகும், இது Self-Attention-ன் வெளியீட்டை மேம்படுத்துகிறது. FFN பொதுவாக இரண்டு layers-ஐ கொண்டிருக்கிறது:

1. **முதல் Layer:** Linear transformation மற்றும் activation function (ReLU போன்றது).
2. **இரண்டாவது Layer:** Linear transformation.

FFN-ன் வெளியீடு Encoder layer-ன் இறுதி வெளியீடாகும்.

---

**Encoder-ன் பணி:**

Encoder-ன் முக்கிய பணி உரையில் உள்ள வார்த்தைகளுக்கிடையேயான உறவுகளை புரிந்துகொள்வது மற்றும் அவற்றின் அர்த்தத்தை பிரதிநிதித்துவப்படுத்துவது. இது பல Encoder layers-ஐ கொண்டிருக்கலாம், ஒவ்வொரு layer-உம் Self-Attention மற்றும் FFN-ஐ பயன்படுத்தி உரையை மேம்படுத்துகிறது.

---

### **3. குறிவிலக்கி (Decoder)**

Decoder என்பது Transformers மாதிரியின் இரண்டாவது முக்கிய பகுதியாகும். இது Encoder-ன் embeddings-ஐ பயன்படுத்தி, வெளியீட்டை (Output) உருவாக்குகிறது. Decoder-ன் பணி என்னவென்றால், Encoder-ல் இருந்து பெறப்பட்ட தகவல்களை பயன்படுத்தி, இலக்கு மொழியில் (Target Language) உரையை உருவாக்குவது அல்லது மொழிபெயர்ப்பது. ஒவ்வொரு Decoder layer-உம் மூன்று முக்கிய பகுதிகளை கொண்டிருக்கிறது:

1. **Masked Multi-Head Self-Attention**
2. **Encoder-Decoder Attention**
3. **Feedforward Neural Network**

---

#### **3.1 Masked Multi-Head Self-Attention:**

Decoder-ல், Self-Attention-ஐ பயன்படுத்தும் போது, ஒரு முக்கியமான வித்தியாசம் உள்ளது. Decoder-ல், **future tokens** (எதிர்கால வார்த்தைகள்) கணக்கில் எடுத்துக்கொள்ளப்படுவதில்லை. இதற்கு **Masking** என்ற ஒரு முறை பயன்படுத்தப்படுகிறது.

- **Masking என்ன?**
  - Masking என்பது ஒரு வழி, இதில் Decoder-ல் ஒரு வார்த்தை அதன் பின்னால் வரும் வார்த்தைகளை பார்க்க முடியாது. எடுத்துக்காட்டாக, "I love cats" என்ற வாக்கியத்தை உருவாக்கும் போது, "love" என்ற வார்த்தை "cats" என்ற வார்த்தையை பார்க்க முடியாது. இது மாதிரியை தற்போதைய வார்த்தையை மட்டுமே பயன்படுத்தி, எதிர்கால வார்த்தைகளை ஊகிக்க உதவுகிறது.
  - இந்த masking செயல்முறை Self-Attention-ல் பயன்படுத்தப்படுகிறது, இதனால் Decoder தற்போதைய வார்த்தையை மட்டுமே பயன்படுத்தி, எதிர்கால வார்த்தைகளை கணிக்க முடியும்.

- **Masked Multi-Head Attention:**
  - Encoder-ல் உள்ளதைப் போல, Decoder-லும் Multi-Head Attention பயன்படுத்தப்படுகிறது. ஆனால், இங்கு masking செயல்முறை சேர்க்கப்படுகிறது.
  - இது Decoder-க்கு தற்போதைய வார்த்தையை மட்டுமே பயன்படுத்தி, எதிர்கால வார்த்தைகளை ஊகிக்க உதவுகிறது.

---

#### **3.2 Encoder-Decoder Attention:**

Encoder-ன் embeddings-ஐ Decoder-ல் பயன்படுத்தி, வெளியீட்டை உருவாக்குகிறது. இது Encoder மற்றும் Decoder-க்கு இடையேயான தொடர்பை ஏற்படுத்துகிறது.

- **Encoder-Decoder Attention-ன் வேலை:**
  - இந்த பகுதியில், Decoder-ல் உள்ள Query ($( \mathbf{Q} )$) Encoder-ல் இருந்து பெறப்படும் Key ($( \mathbf{K} )$) மற்றும் Value ($( \mathbf{V} )$)-ஐ பயன்படுத்தி, attention scores கணக்கிடப்படுகிறது.
  - இது Decoder-க்கு Encoder-ல் இருந்து பெறப்பட்ட தகவல்களை பயன்படுத்தி, சரியான வெளியீட்டை உருவாக்க உதவுகிறது.
  - எடுத்துக்காட்டாக, மொழிபெயர்ப்பில், Encoder-ல் உள்ள உரை தகவல்களை Decoder பயன்படுத்தி, இலக்கு மொழியில் உரையை உருவாக்குகிறது.

- **Query, Key, மற்றும் Value:**
  - Query ($( \mathbf{Q} )$) Decoder-ல் இருந்து பெறப்படுகிறது.
  - Key ($( \mathbf{K} )$) மற்றும் Value ($( \mathbf{V} )$) Encoder-ல் இருந்து பெறப்படுகிறது.
  - இந்த மூன்று வெக்டர்களும் attention mechanism-ஐ பயன்படுத்தி, Encoder மற்றும் Decoder-க்கு இடையேயான தொடர்பை ஏற்படுத்துகின்றன.

---

#### **3.3 Feedforward Neural Network:**

Encoder-ல் உள்ளதைப் போல, Decoder-லும் Feedforward Neural Network (FFN) பயன்படுத்தப்படுகிறது. இது Self-Attention மற்றும் Encoder-Decoder Attention-ன் வெளியீட்டை மேலும் செயலாக்குகிறது.

- **Feedforward Neural Network-ன் வேலை:**
  - FFN என்பது ஒரு எளிய நரம்பியல் வலைப்பின்னல் (Neural Network) ஆகும், இது இரண்டு layers-ஐ கொண்டிருக்கிறது:
    1. **முதல் Layer:** Linear transformation மற்றும் activation function (ReLU போன்றது).
    2. **இரண்டாவது Layer:** Linear transformation.
  - இது Decoder-ன் வெளியீட்டை மேம்படுத்துகிறது மற்றும் இறுதி வெளியீட்டை உருவாக்க உதவுகிறது.

---

**Decoder-ன் பணி:**

Decoder-ன் முக்கிய பணி Encoder-ல் இருந்து பெறப்பட்ட தகவல்களை பயன்படுத்தி, இலக்கு மொழியில் உரையை உருவாக்குவது. இது பல Decoder layers-ஐ கொண்டிருக்கலாம், ஒவ்வொரு layer-உம் Masked Self-Attention, Encoder-Decoder Attention, மற்றும் FFN-ஐ பயன்படுத்தி உரையை மேம்படுத்துகிறது. இறுதியில், Decoder-ன் வெளியீடு இலக்கு மொழியில் உரையாக மாற்றப்படுகிறது.

---

நாம் ஒரு உதாரணத்தை பயன்படுத்தி, **Encoder** மற்றும் **Decoder**-ன் செயல்பாட்டை விரிவாக காண்போம். உதாரணத்திற்கு நாம் ஒரு எளிய மொழிபெயர்ப்பு பணியை எடுத்துக்கொள்வோம். உள்ளீடு (Input) ஆங்கிலத்தில் "I love cats" என்று இருக்கும், மற்றும் வெளியீடு (Output) தமிழில் "நான் பூனைகளை விரும்புகிறேன்" என்று இருக்கும்.

### **Encoder-ன் செயல்பாடு:**

#### **1. உள்ளீட்டு பிரதிநிதித்துவம் (Input Representation):**
- உள்ளீடு "I love cats" என்பது முதலில் embeddings-ஆக மாற்றப்படுகிறது. ஒவ்வொரு வார்த்தையும் ஒரு வெக்டராக (Vector) மாற்றப்படுகிறது.
  - "I" → $( \mathbf{e}_1 )$
  - "love" → $( \mathbf{e}_2 )$
  - "cats" → $( \mathbf{e}_3 )$
- பின்னர், இந்த embeddings-களுடன் **Positional Encoding** சேர்க்கப்படுகிறது. இது வார்த்தைகளின் நிலை (Position) பற்றிய தகவலை சேமிக்கிறது.
  - "I" (Position 1) → $( \mathbf{e}_1 + \mathbf{P}_1 )$
  - "love" (Position 2) → $( \mathbf{e}_2 + \mathbf{P}_2 )$
  - "cats" (Position 3) → $( \mathbf{e}_3 + \mathbf{P}_3 )$

#### **2. Multi-Head Self-Attention:**
- Encoder-ல், Self-Attention மூலம் ஒவ்வொரு வார்த்தையும் மற்ற வார்த்தைகளுடன் எவ்வாறு தொடர்புடையது என்பதை புரிந்துகொள்கிறது.
  - எடுத்துக்காட்டாக, "love" என்ற வார்த்தை "I" மற்றும் "cats" உடன் எவ்வாறு தொடர்புடையது என்பதை கணக்கிடுகிறது.
  - இதற்கு Query ($( \mathbf{Q} )$), Key ($( \mathbf{K} )$), மற்றும் Value ($( \mathbf{V} )$) வெக்டர்கள் பயன்படுத்தப்படுகின்றன.
  - Attention scores கணக்கிடப்பட்டு, softmax மூலம் நிகழ்தகவுகளாக மாற்றப்படுகின்றன.
  - இறுதியில், weighted sum மூலம் ஒவ்வொரு வார்த்தையின் புதிய பிரதிநிதித்துவம் கணக்கிடப்படுகிறது.

#### **3. Feedforward Neural Network:**
- Self-Attention-ன் வெளியீடு Feedforward Neural Network (FFN)-ல் செயலாக்கப்படுகிறது. இது உரையின் அர்த்தத்தை மேலும் மேம்படுத்துகிறது.
- இறுதியில், Encoder-ன் வெளியீடு ஒரு தொகுப்பு embeddings-ஆகும், இது உரையின் அர்த்தத்தை பிரதிநிதித்துவப்படுத்துகிறது.

---

### **Decoder-ன் செயல்பாடு:**

#### **1. Masked Multi-Head Self-Attention:**
- Decoder-ல், முதலில் "நான்" என்ற வார்த்தை மட்டுமே உள்ளது. இது Self-Attention-ஐ பயன்படுத்தி, தற்போதைய வார்த்தையை மட்டுமே பார்க்கிறது (ஏனெனில் எதிர்கால வார்த்தைகள் இன்னும் உருவாக்கப்படவில்லை).
  - "நான்" → $( \mathbf{e}_1 + \mathbf{P}_1 )$
  - இங்கு, masking செயல்முறை "பூனைகளை" மற்றும் "விரும்புகிறேன்" போன்ற எதிர்கால வார்த்தைகளை பார்க்காமல், தற்போதைய வார்த்தையை மட்டுமே பயன்படுத்துகிறது.

#### **2. Encoder-Decoder Attention:**
- Decoder, Encoder-ல் இருந்து பெறப்பட்ட embeddings-ஐ பயன்படுத்தி, "நான்" என்ற வார்த்தைக்கு சரியான வெளியீட்டை உருவாக்குகிறது.
  - Query ($( \mathbf{Q} )$) Decoder-ல் இருந்து பெறப்படுகிறது ("நான்").
  - Key ($( \mathbf{K} )$) மற்றும் Value ($( \mathbf{V} )$) Encoder-ல் இருந்து பெறப்படுகின்றன ("I", "love", "cats").
  - Attention scores கணக்கிடப்பட்டு, Encoder-ன் embeddings-ஐ பயன்படுத்தி, "நான்" என்ற வார்த்தைக்கு சரியான வெளியீடு உருவாக்கப்படுகிறது.

#### **3. Feedforward Neural Network:**
- Encoder-Decoder Attention-ன் வெளியீடு Feedforward Neural Network (FFN)-ல் செயலாக்கப்படுகிறது. இது வெளியீட்டை மேலும் மேம்படுத்துகிறது.
- இறுதியில், Decoder-ன் வெளியீடு "நான்" என்ற வார்த்தையாக உருவாகிறது.

---

### **முழு செயல்முறை:**

1. **Encoder:**
   - உள்ளீடு "I love cats" என்பது embeddings-ஆக மாற்றப்பட்டு, Self-Attention மற்றும் FFN மூலம் செயலாக்கப்படுகிறது.
   - Encoder-ன் வெளியீடு ஒரு தொகுப்பு embeddings-ஆகும், இது உரையின் அர்த்தத்தை பிரதிநிதித்துவப்படுத்துகிறது.

2. **Decoder:**
   - Decoder முதலில் "நான்" என்ற வார்த்தையை உருவாக்குகிறது. இது Masked Self-Attention மற்றும் Encoder-Decoder Attention-ஐ பயன்படுத்தி, Encoder-ன் embeddings-ஐ பயன்படுத்துகிறது.
   - பின்னர், "பூனைகளை" மற்றும் "விரும்புகிறேன்" போன்ற வார்த்தைகளை ஒவ்வொன்றாக உருவாக்குகிறது.
   - ஒவ்வொரு வார்த்தையும் Encoder-ன் embeddings-ஐ பயன்படுத்தி, சரியான வெளியீட்டை உருவாக்குகிறது.

இந்த செயல்முறை மூலம், Transformers மாதிரி உரையை புரிந்துகொண்டு, மொழிபெயர்ப்பு போன்ற பணிகளை திறம்பட செயல்படுத்துகிறது.

### **4. வெளியீட்டு இழை (Output Layer)**

Decoder-ன் வெளியீடு ஒரு linear transformation மற்றும் softmax செயல்பாடு மூலம் வெளியீட்டு நிகழ்தகவுகளாக (Output Probabilities) மாற்றப்படுகிறது. இந்த செயல்முறை மாதிரியை இலக்கு மொழியில் உரையை உருவாக்க உதவுகிறது.

#### **வெளியீட்டு அடுக்கு செயல்முறை:**

1. **Linear Transformation:**
   - Decoder-ன் வெளியீடு $( \mathbf{h} )$ (ஒரு வெக்டர்) ஒரு linear transformation-ஐ மூலம் செயலாக்கப்படுகிறது. இது ஒரு weight matrix $( \mathbf{W}_o )$ மற்றும் bias vector $( \mathbf{b}_o )$ பயன்படுத்தி கணக்கிடப்படுகிறது:
     $$
     [
     \mathbf{z} = \mathbf{W}_o \mathbf{h} + \mathbf{b}_o
     ]
     $$
     இங்கு $( \mathbf{z} )$ என்பது linear transformation-ன் வெளியீடு.
   
2. **Softmax செயல்பாடு:**
   - Linear transformation-ன் வெளியீடு $( \mathbf{z} )$ softmax செயல்பாடு மூலம் நிகழ்தகவுகளாக (Probabilities) மாற்றப்படுகிறது. இது ஒவ்வொரு வார்த்தையின் நிகழ்தகவை கணக்கிடுகிறது:
     $$
     [
     \text{Output} = \text{Softmax}(\mathbf{z})
     ]
     $$
     Softmax செயல்பாடு ஒவ்வொரு வார்த்தையின் நிகழ்தகவை 0 மற்றும் 1-க்கு இடையில் இருக்கும் வகையில் மாற்றுகிறது, மேலும் அனைத்து நிகழ்தகவுகளின் கூட்டுத்தொகை 1 ஆக இருக்கும்.
   
3. **வெளியீட்டு நிகழ்தகவுகள்:**
   - Softmax-ன் வெளியீடு ஒரு நிகழ்தகவு விநியோகம் (Probability Distribution) ஆகும். இது ஒவ்வொரு வார்த்தையின் நிகழ்தகவை குறிக்கிறது. எடுத்துக்காட்டாக, "நான்", "பூனைகளை", "விரும்புகிறேன்" போன்ற வார்த்தைகளின் நிகழ்தகவுகள் கணக்கிடப்படுகின்றன.
   - மாதிரி இந்த நிகழ்தகவுகளை பயன்படுத்தி, அடுத்த வார்த்தையை தேர்ந்தெடுக்கிறது.

---

### **5. Layer Normalization மற்றும் Residual Connections:**

Transformers-ல், **Layer Normalization** மற்றும் **Residual Connections** பயன்படுத்தப்படுகின்றன. இவை மாதிரியின் பயிற்சியை மேம்படுத்துகின்றன மற்றும் மாதிரியின் செயல்திறனை அதிகரிக்கின்றன.

#### **Layer Normalization:**

Layer Normalization என்பது ஒரு நெறிமுறை (Normalization Technique) ஆகும், இது மாதிரியின் ஒவ்வொரு layer-ன் வெளியீட்டையும் நெறிப்படுத்த (Normalize) உதவுகிறது. இது மாதிரியின் பயிற்சியை வேகமாகவும், நிலையாகவும் மேம்படுத்துகிறது.

- **Layer Normalization செயல்முறை:**
  - ஒரு layer-ன் வெளியீடு $( x )$ என்பது mean \( \mu )$ மற்றும் variance \( \sigma^2 )$ பயன்படுத்தி நெறிப்படுத்தப்படுகிறது:
    $$
    [
    \text{LayerNorm}(x) = \gamma \cdot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta
    ]
    $$
    இங்கு:
    
    - $( \mu )$ என்பது mean (சராசரி).
    - $( \sigma^2 )$ என்பது variance (பரவல்).
    - $( \gamma )$ மற்றும் $( \beta )$ என்பது trainable parameters (பயிற்சி மூலம் கற்றுக்கொள்ளப்படும் எடைகள்).
    - $( \epsilon )$ என்பது ஒரு சிறிய மதிப்பு, இது பூஜ்ஜியத்தால் வகுத்தலை தவிர்க்க உதவுகிறது.
  
- **பயன்:**
  - Layer Normalization மாதிரியின் ஒவ்வொரு layer-ன் வெளியீட்டையும் நிலையான (Stable) மற்றும் சீரான (Consistent) வடிவில் வைத்திருக்க உதவுகிறது. இது மாதிரியின் பயிற்சியை வேகமாகவும், நிலையாகவும் மேம்படுத்துகிறது.

#### **Residual Connections:**

Residual Connections என்பது ஒரு முக்கியமான கருத்தாகும், இது மாதிரியின் ஒவ்வொரு layer-ன் வெளியீட்டை அதன் உள்ளீட்டுடன் நேரடியாக சேர்க்கிறது. இது மாதிரியின் பயிற்சியை மேம்படுத்துகிறது மற்றும் vanishing gradients (சிறிய சாய்வுகள்) பிரச்சினையை தவிர்க்க உதவுகிறது.

- **Residual Connections செயல்முறை:**
  - ஒரு layer-ன் வெளியீடு $( x )$ என்பது அதன் உள்ளீட்டுடன் நேரடியாக சேர்க்கப்படுகிறது:
    $$
    [
    \text{Output} = x + \text{Sublayer}(x)
    ]
    $$
    இங்கு $( \text{Sublayer} )$ என்பது Self-Attention அல்லது Feedforward Network.
  
- **பயன்:**
  - Residual Connections மாதிரியின் ஒவ்வொரு layer-ன் வெளியீட்டையும் அதன் உள்ளீட்டுடன் சேர்க்கிறது. இது மாதிரியின் பயிற்சியை மேம்படுத்துகிறது மற்றும் vanishing gradients பிரச்சினையை தவிர்க்க உதவுகிறது.

---

### **6. Transformers-ன் பயிற்சி (Training the Transformer):**

Transformers-ன் பயிற்சி **Cross-Entropy Loss** மூலம் செய்யப்படுகிறது. இது predicted output மற்றும் actual output-க்கு இடையேயான வித்தியாசத்தை கணக்கிடுகிறது.

#### **Cross-Entropy Loss:**

Cross-Entropy Loss என்பது ஒரு loss function ஆகும், இது மாதிரியின் predicted output மற்றும் actual output-க்கு இடையேயான வித்தியாசத்தை கணக்கிடுகிறது. இது மாதிரியின் பயிற்சியை மேம்படுத்த உதவுகிறது.

- **Cross-Entropy Loss செயல்முறை:**
  - Actual output $( y_i )$ மற்றும் predicted output $( \hat{y}_i )$ பயன்படுத்தி, loss கணக்கிடப்படுகிறது:
    $$
    [
    \text{Loss} = -\sum_{i=1}^n y_i \log(\hat{y}_i)
    ]
    $$
    இங்கு:
    
    - $( y_i )$ என்பது actual output (உண்மையான வெளியீடு).
    - $( \hat{y}_i )$ என்பது predicted output (மாதிரியின் ஊகம்).
  
- **பயன்:**
  - Cross-Entropy Loss மாதிரியின் predicted output மற்றும் actual output-க்கு இடையேயான வித்தியாசத்தை கணக்கிடுகிறது. இது மாதிரியின் பயிற்சியை மேம்படுத்த உதவுகிறது.

Transformers மாதிரி உரையை புரிந்துகொண்டு, மொழிபெயர்ப்பு போன்ற பணிகளை திறம்பட செயல்படுத்துகிறது. இது Encoder, Decoder, Layer Normalization, Residual Connections, மற்றும் Cross-Entropy Loss போன்ற கருத்துகளை பயன்படுத்தி, உரையை புரிந்துகொண்டு, வெளியீட்டை உருவாக்குகிறது. இந்த கருத்துகள் Transformers மாதிரியின் செயல்திறனை மேம்படுத்துகின்றன மற்றும் அதை பல NLP பணிகளில் பயன்படுத்த உதவுகின்றன.