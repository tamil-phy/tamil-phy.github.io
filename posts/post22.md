## **ஆட்டோமேடிக் டிஃபரன்ஷியேஷன்**

ஆட்டோமேடிக் டிஃபரன்ஷியேஷன் என்பது கணித செயல்பாடுகளின் கிரேடியன்ட்களை தானாகவே கணக்கிடும் ஒரு முறை. இதைப் புரிந்துகொள்ள, முதலில் சில அடிப்படை விஷயங்களைப் பார்க்கலாம்.

உதாரணமாக y = 2x + 3 என்ற சமன்பாட்டை எடுத்துக்கொள்வோம். 

இங்கு:

- x என்பது இன்புட் (உள்ளீடு)
- 2 என்பது வெயிட் (எடை)
- 3 என்பது பயஸ் (சாய்வு)
- y என்பது அவுட்புட் (வெளியீடு)

**கிரேடியன்ட் என்றால் என்ன?**

கிரேடியன்ட் என்ற கருத்தை முழுமையாகப் புரிந்துகொள்ள, நாம் ஒரு உண்மையான உலக உதாரணத்தில் இருந்து தொடங்குவோம்:

**உதாரணம் 1: கார் பயணம்**

- நீங்கள் ஒரு காரில் பயணிக்கிறீர்கள் என்று வைத்துக்கொள்வோம்
- நேரம் (t): 1 மணி → 2 மணி
- தூரம் (d): 50 கிமீ → 80 கிமீ
- வேகம் = தூரத்தின் மாற்றம் / நேரத்தின் மாற்றம்
- (80-50)/(2-1) = 30 கிமீ/மணி

இங்கே, 30 கிமீ/மணி என்பது "நேரம் மாறும்போது தூரம் எவ்வளவு மாறுகிறது" என்பதைக் காட்டுகிறது. இதைத்தான் கணிதத்தில் "வழித்தோன்றல்" (Derivative) என்று சொல்கிறோம்.

**உதாரணம் 2: கணிதச் சமன்பாடு**
y = 2x + 3 என்ற சமன்பாட்டை எடுத்துக்கொள்வோம்:

| x    | y    |
| ---- | ---- |
| 1    | 5    |
| 2    | 7    |
| 3    | 9    |

இங்கே:
- x 1-ல் இருந்து 2-ஆக மாறும் போது (Δx = 1)
- y 5-ல் இருந்து 7-ஆக மாறுகிறது (Δy = 2)
- கிரேடியன்ட் = Δy/Δx = 2/1 = 2

இதை கணித ரீதியாக ∂y/∂x = 2 என்று எழுதுகிறோம்.

**ஏன் இது முக்கியம்?**
நியூரல் நெட்வொர்க்குகளில், நாம் தெரிந்துகொள்ள வேண்டியது:
- ஒவ்வொரு வெயிட் (w) மற்றும் பயஸ் (b) மாறினால் லாஸ் எவ்வளவு மாறுகிறது
- எந்த திசையில் மாற்றம் செய்ய வேண்டும் (அதிகரிக்க/குறைக்க)

**மேலும் விரிவான பார்வை:**
1. **ஒற்றை மாறி:** y = f(x)
   - கிரேடியன்ட் = dy/dx

2. **பல மாறிகள்:** z = f(x,y)
   - ∂z/∂x (y-ஐ மாறிலியாக வைத்து)
   - ∂z/∂y (x-ஐ மாறிலியாக வைத்து)

**PyTorch உதாரணம்:**

```python
import torch

x = torch.tensor(3.0, requires_grad=True)
y = 2*x**2 + 3*x + 1

y.backward()

print(x.grad)  # dy/dx = 4x + 3 = 15 (x=3 ஆக இருக்கும் போது)
```

**PyTorch-இல் இதை எப்படி செய்வது?**

```python
import torch

# டென்சர்களை உருவாக்குதல்
x = torch.tensor(1.0, requires_grad=True)  # requires_grad=True என்றால் கிரேடியன்ட் கணக்கிடு
w = torch.tensor(2.0, requires_grad=True)
b = torch.tensor(3.0, requires_grad=True)

# கணக்கீடு
y = w * x + b

# கிரேடியன்ட் கணக்கிடுதல்
y.backward()

# கிரேடியன்ட்களைப் பார்ப்பது
print("dy/dx:", x.grad)  # 2.0
print("dy/dw:", w.grad)  # 1.0
print("dy/db:", b.grad)  # 1.0
```

**இதன் விளக்கம்:**

- `y.backward()` என்று அழைக்கும்போது, PyTorch பின்வருவனவற்றை செய்கிறது:
  1. y = w*x + b என்ற கணக்கை ஞாபகப்படுத்துகிறது
  2. ஒவ்வொரு மாறிக்கும் (x, w, b) கிரேடியன்ட் கணக்கிடுகிறது
  3. இந்த கிரேடியன்ட்களை .grad என்ற பெயரில் சேமிக்கிறது

**ஏன் இது முக்கியம்?**
நியூரல் நெட்வொர்க்குகளை பயிற்றுவிக்கும்போது (train செய்யும்போது), ஒவ்வொரு வெயிட் மற்றும் பயஸ் எவ்வளவு மாற்றம் வேண்டும் என்பதை இந்த கிரேடியன்ட்கள் சொல்கின்றன.

```python
# சிக்மாய்டு செயல்பாடு
def sigmoid(x):
    return 1 / (1 + torch.exp(-x))

# டேட்டா
x = torch.tensor([0.5, -0.3, 1.2], requires_grad=True)
w = torch.tensor([0.2, -0.1, 0.4], requires_grad=True)
b = torch.tensor(0.1, requires_grad=True)

# நியூரல் நெட்வொர்க்
z = torch.dot(x, w) + b  # டாட் ப்ராடக்ட்
a = sigmoid(z)           # ஆக்டிவேஷன்
loss = (a - 0.5)**2      # லாஸ்

# கிரேடியன்ட் கணக்கீடு
loss.backward()

print("d(loss)/dw:", w.grad)
print("d(loss)/db:", b.grad)
```

#### **பின்னோக்கி பரப்புதல் (Backpropagation)**

**பின்னோக்கி பரப்புதல் (Backpropagation)** என்பது **நியூரல் நெட்வொர்க்குகளை (Neural Networks)** பயிற்சியளிக்கப் பயன்படும் மிக முக்கியமான **அல்காரிதம் (Algorithm)** ஆகும். இது **கிரேடியன்ட் டிசென்ட் (Gradient Descent)** முறையில் **லாஸ் ஃபங்க்ஷன் (Loss Function)** குறைக்க உதவுகிறது. இந்த கட்டுரையில், பின்னோக்கி பரப்புதல் எப்படி வேலை செய்கிறது என்பதை எளிய உதாரணங்களுடன் புரிந்துகொள்வோம்.

---

##### **முன்னோக்கி பாய்ச்சல் (Forward Pass)**

முன்னோக்கி பாய்ச்சல் என்பது தரவு நியூரல் நெட்வொர்க்கின் வழியே சென்று **ப்ரெடிக்ஷன் (Prediction)** செய்யும் செயல்முறை ஆகும். இது பின்வரும் படிகளைக் கொண்டுள்ளது:

##### **இன்புட் லேயர் (Input Layer)**
- உள்ளீட்டு தரவு (Input Data) நெட்வொர்க்கில் நுழைகிறது. 
- எடுத்துக்காட்டாக, ஒரு **28x28 பிக்சல் (Pixel)** உள்ள **MNIST** எழுத்து படத்தில், **784 இன்புட் நியூரான்கள் (Input Neurons)** இருக்கும்.
- இது ஒரு **வெக்டர் (Vector)** அல்லது **மேட்ரிக்ஸ் (Matrix)** வடிவில் இருக்கும்.

##### **வெயிட் பெருக்கல் (Weight Multiplication)**
- ஒவ்வொரு **நியூரான் (Neuron)** அதன் **வெயிட்கள் (Weights)** மூலம் உள்ளீட்டை பெருக்குகிறது.
- கணித ரீதியாக:
  ```python
  z = w1*x1 + w2*x2 + ... + wn*xn + b
  ```
  - `w1, w2, ..., wn` → வெயிட்கள் (Weights)
  - `x1, x2, ..., xn` → உள்ளீடுகள் (Inputs)
  - `b` → பயஸ் (Bias)

##### **ஆக்டிவேஷன் செயல்பாடு (Activation Function)**
- **லீனியர் (Linear)** கணக்கீடுகளுக்கு **நேரியல் அல்லாத (Non-linear)** தன்மையை சேர்க்க **ஆக்டிவேஷன் ஃபங்க்ஷன் (Activation Function)** பயன்படுத்தப்படுகிறது.
- பொதுவான ஆக்டிவேஷன் ஃபங்க்ஷன்கள்:
  - **சிக்மாய்டு (Sigmoid):** `σ(z) = 1 / (1 + e^(-z))` → 0 முதல் 1 வரை வெளியீடு
  - **ReLU (Rectified Linear Unit):** `max(0, z)` → எதிர்மறை மதிப்புகளை 0 ஆக மாற்றுகிறது
  - **டான்h (Tanh):** `tanh(z)` → -1 முதல் 1 வரை வெளியீடு

##### **லாஸ் கணக்கீடு (Loss Calculation)**
- **ப்ரெடிக்ஷன் (Prediction)** உண்மை மதிப்புடன் (Ground Truth) ஒப்பிடப்பட்டு **லாஸ் (Loss)** கணக்கிடப்படுகிறது.
- பொதுவான **லாஸ் ஃபங்க்ஷன்கள் (Loss Functions):**
  - **Mean Squared Error (MSE):** `(y_pred - y_true)²` → ரிக்ரஷன் பிரச்சினைகளுக்கு
  - **Cross-Entropy Loss:** `-Σ(y_true * log(y_pred))` → கிளாஸிஃபிகேஷன் பிரச்சினைகளுக்கு

##### **பின்னோக்கி பாய்ச்சல் (Backward Pass)**

லாஸ் கணக்கிடப்பட்ட பிறகு, **கிரேடியன்ட்கள் (Gradients)** கணக்கிடப்படுகின்றன. இதற்கு **சங்கிலி விதி (Chain Rule)** பயன்படுத்தப்படுகிறது.

##### **லாஸ் லேயரில் கிரேடியன்ட் (Gradient at Loss Layer)**
- லாஸ் ஃபங்க்ஷனின் வழித்தோன்றல் கணக்கிடப்படுகிறது:
  ```
  ∂loss/∂y_pred
  ```
- எடுத்துக்காட்டாக, **MSE லாஸ்**-க்கு:
  ```
  ∂loss/∂y_pred = 2*(y_pred - y_true)
  ```

##### **ஆக்டிவேஷன் லேயரில் கிரேடியன்ட் (Gradient at Activation Layer)**
- ஆக்டிவேஷன் செயல்பாட்டின் வழித்தோன்றல் கணக்கிடப்படுகிறது:
  ```
  ∂loss/∂z = (∂loss/∂a) * (∂a/∂z)
  ```
- **சிக்மாய்டு** ஆக்டிவேஷனுக்கு:
  ```
  ∂a/∂z = σ(z) * (1 - σ(z))
  ```

##### **வெயிட் லேயரில் கிரேடியன்ட் (Gradient at Weight Layer)**
- ஒவ்வொரு **வெயிட்டுக்கும் (Weight)** கிரேடியன்ட் கணக்கிடப்படுகிறது:
  ```
  ∂loss/∂w = (∂loss/∂z) * (∂z/∂w) = (∂loss/∂z) * x
  ```
- இது **ஒவ்வொரு வெயிட்டும் எவ்வளவு மாற்றம் வேண்டும்** என்பதைக் காட்டுகிறது.

##### **பயஸ் கிரேடியன்ட் (Bias Gradient)**
- பயஸின் கிரேடியன்ட்:
  ```
  ∂loss/∂b = (∂loss/∂z) * (∂z/∂b) = (∂loss/∂z) * 1
  ```

##### **இன்புட் லேயர் வரை கிரேடியன்ட் பரவுதல் (Gradient Propagation to Input Layer)**
- கிரேடியன்ட்கள் **முழு நெட்வொர்க்கின் மூலமாகவும் பின்னோக்கி பரவுகின்றன**:
  ```
  ∂loss/∂x = (∂loss/∂z) * (∂z/∂x) = (∂loss/∂z) * w
  ```

---

##### **கிரேடியன்ட் டிசென்ட் (Gradient Descent)**

கிரேடியன்ட்கள் கிடைத்தவுடன், **வெயிட்கள் மற்றும் பயஸை அப்டேட் (Update)** செய்யலாம்:
```
w_new = w_old - η * (∂loss/∂w)
b_new = b_old - η * (∂loss/∂b)
```
- `η` → **கற்றல் விகிதம் (Learning Rate)** (எவ்வளவு வேகமாக மாற்றம் செய்ய வேண்டும் என்பதை தீர்மானிக்கிறது)

---

##### **PyTorch-இல் பின்னோக்கி பரப்புதல்**

PyTorch-இல், **Autograd** எனப்படும் **ஆட்டோமேடிக் டிஃபரன்ஷியேஷன் (Automatic Differentiation)** இந்த முழு செயல்முறையையும் தானாகவே கவனித்துக்கொள்கிறது.

###### **எடுத்துக்காட்டு:**
```python
import torch
import torch.nn.functional as F

# டேட்டா தயாரித்தல்
x = torch.tensor([1.0], requires_grad=True)
w = torch.tensor([2.0], requires_grad=True)
b = torch.tensor([0.5], requires_grad=True)

# Forward Pass
z = x * w + b
a = torch.sigmoid(z)
loss = F.binary_cross_entropy(a, torch.tensor([1.0]))

# Backward Pass (Autograd)
loss.backward()

# Gradients
print("∂loss/∂w:", w.grad)  # -0.1966
print("∂loss/∂b:", b.grad)  # -0.1966
```

---

பின்னோக்கி பரப்புதல் என்பது **நியூரல் நெட்வொர்க்குகளின் (Neural Networks)** மையமான கருத்தாகும். இது **லாஸ் ஃபங்க்ஷனை (Loss Function)** குறைக்க **கிரேடியன்ட்களை (Gradients)** திறம்பட கணக்கிடுகிறது. **PyTorch** மற்றும் **TensorFlow** போன்ற லைப்ரரிகள் இதை தானாகவே செய்கின்றன, ஆனால் இதன் அடிப்படைக் கருத்துக்களைப் புரிந்துகொள்வது **டீப் லர்னிங் (Deep Learning)** மாடல்களை சரியாக புரிந்துகொள்ள உதவுகிறது. 

#### லீனியர் ரிக்ரஷன்:

```python
# டேட்டா தயாரித்தல்
X = torch.tensor([[1.0], [2.0], [3.0]])
y = torch.tensor([[2.0], [4.0], [6.0]])

# மாடல் பராமீட்டர்கள்
w = torch.tensor([[0.0]], requires_grad=True)
b = torch.tensor([[0.0]], requires_grad=True)

# டிரெயினிங் லூப்
learning_rate = 0.01
for epoch in range(100):
    # ஃபார்வார்ட் பாஸ்
    y_pred = X @ w + b
    
    # லாஸ்
    loss = ((y_pred - y)**2).mean()
    
    # பேக்வர்ட் பாஸ்
    loss.backward()
    
    # பராமீட்டர் அப்டேட்
    with torch.no_grad():
        w -= learning_rate * w.grad
        b -= learning_rate * b.grad
        
        # கிரேடியன்ட்களை ரீசெட் செய்தல்
        w.grad.zero_()
        b.grad.zero_()
```

**முக்கியமான குறிப்புகள் மற்றும் பொதுவான தவறுகளின் விளக்கம்:**

**கிரேடியன்ட் கணக்கீட்டிற்கான முக்கிய கட்டளைகள்:**

`requires_grad=True` என்பது PyTorch-க்கு "இந்த டென்சருக்கான கிரேடியன்ட்களை கண்காணி" என்று சொல்கிறது. எடுத்துக்காட்டாக, நீங்கள் ஒரு நியூரல் நெட்வொர்க் வெயிட்களை உருவாக்கும்போது இதை அமைக்க வேண்டும், ஏனெனில் பயிற்சி செயல்பாட்டில் இந்த வெயிட்கள் எவ்வாறு மாற்றப்பட வேண்டும் என்பதை கிரேடியன்ட்கள் தீர்மானிக்கின்றன.

`backward()` முறையை அழைக்கும்போது, PyTorch தானாகவே கம்ப்யூடேஷனல் கிராபை பின்னோக்கி பயணித்து அனைத்து கிரேடியன்ட்களையும் கணக்கிடுகிறது. இந்த செயல்முறை backpropagation என்று அழைக்கப்படுகிறது, இது நியூரல் நெட்வொர்க்குகளின் பயிற்சியின் மையமாகும்.

`zero_grad()` முறை மிகவும் முக்கியமானது, ஏனெனில் PyTorch-இல் கிரேடியன்ட்கள் தானாகவே கூடுகின்றன. நீங்கள் இதை அழைக்கவில்லை என்றால், ஒவ்வொரு முறை backward() அழைக்கப்படும் போதும் புதிய கிரேடியன்ட்கள் முந்தையவற்றுடன் சேர்க்கப்படும், இது தவறான பயிற்சி முடிவுகளுக்கு வழிவகுக்கும்.

`with torch.no_grad():` தொகுதி PyTorch-க்கு "இந்த பகுதியில் கிரேடியன்ட் கணக்கீடு தேவையில்லை" என்று சொல்கிறது. இது பொதுவாக பராமீட்டர் அப்டேட்கள் செய்யும் போது அல்லது மாதிரியை மதிப்பிடும் போது பயன்படுத்தப்படுகிறது, ஏனெனில் இந்த செயல்பாடுகளுக்கு கிரேடியன்ட் கணக்கீடு தேவையில்லை.

**பொதுவாக செய்யப்படும் தவறுகள்:**

`zero_grad()` முறையை மறந்துவிடுவது ஒரு பொதுவான தவறாகும். இது இல்லாவிட்டால், கிரேடியன்ட்கள் தவறாக கூடுகின்றன, இது மாதிரியின் பயிற்சியை முற்றிலும் பாதிக்கும். எடுத்துக்காட்டாக, ஒவ்வொரு எபோக்கிற்கும் முன்பு optimizer.zero_grad() அழைக்கப்பட வேண்டும்.

`requires_grad` சரியாக அமைக்கப்படாத போது, PyTorch அந்த டென்சருக்கான கிரேடியன்ட்களை கணக்கிடாது. இது பெரும்பாலும் புதியவர்கள் செய்யும் தவறு, இதன் விளைவாக மாதிரி பயிற்சி பெறுவதில்லை அல்லது பிழைகள் ஏற்படுகின்றன.

`backward()` முறையை அழைக்கும்போது லாஸ் ஒரு ஸ்கேலர் மதிப்பாக (ஒற்றை எண்) இருக்க வேண்டும். நீங்கள் பல மதிப்புகள் கொண்ட டென்சருடன் backward() அழைத்தால், PyTorch எப்படி கிரேடியன்ட்களை கணக்கிட வேண்டும் என தெரியாமல் குழப்பமடையும், இது பிழையை ஏற்படுத்தும். இதைத் தவிர்க்க, loss.mean() அல்லது loss.sum() போன்ற முறைகளைப் பயன்படுத்தி லாஸை ஒரு ஸ்கேலராக மாற்றலாம்.

PyTorch-இன் ஆட்டோமேடிக் டிஃபரன்ஷியேஷன் அமைப்பு, நியூரல் நெட்வொர்க்குகளை பயிற்றுவிப்பதை மிகவும் எளிதாக்குகிறது. இந்த அடிப்படை கருத்துக்களைப் புரிந்துகொண்டால், நீங்கள் மேலும் சிக்கலான மாடல்களை உருவாக்கத் தொடங்கலாம்!