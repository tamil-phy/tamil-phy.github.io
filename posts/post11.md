## Word2Vec:

Word2Vec என்பது **வார்த்தைகளை வெக்டர்களாக (vectors) மாற்றும் ஒரு தொழில்நுட்பம்** ஆகும். இது வார்த்தைகளுக்கு இடையேயான உறவுகளை (relationships) ஒரு வரைபடம் (graph) போன்று பிரதிநிதித்துவப்படுத்துகிறது. இந்த தொழில்நுட்பம் மெஷின் கற்றல் (machine learning) மற்றும் உரை பகுப்பாய்வு (text analysis) போன்ற துறைகளில் பரவலாக பயன்படுத்தப்படுகிறது.

2013-ல் Google தங்கள் தேடுபொறிக்காக (search engine) Word2Vec-ஐ அறிமுகப்படுத்தியது, மேலும் இந்த அல்காரிதத்தை பேட்டன்ட் செய்தது. இந்த தொழில்நுட்பத்தை Tomas Mikolov மற்றும் அவரது குழுவினர் உருவாக்கினர். இந்த கட்டுரையில், Word2Vec-ஐப் பயன்படுத்தி embeddings உருவாக்கும் கருத்து மற்றும் செயல்முறைகளை புரிந்து கொள்வோம்.

---

### **Word Embedding என்றால் என்ன?**
உரை பகுப்பாய்வில், ஒரு வார்த்தையை பிரதிநிதித்துவப்படுத்த **Word Embedding** பயன்படுத்தப்படுகிறது. இது பொதுவாக ஒரு திசையன் (vector) வடிவில் இருக்கும், இது வார்த்தையின் அர்த்தத்தை குறியாக்கம் (encode) செய்கிறது. இந்த திசையன் வெளி (vector space) ஒரே மாதிரியான அர்த்தம் கொண்ட வார்த்தைகள் ஒன்றுக்கொன்று அருகில் இருக்கும்.

உதாரணம்:

- **"ராஜா" (King) மற்றும் "ராணி" (Queen) என்ற வார்த்தைகள் ஒரே மாதிரியான embeddings கொண்டிருக்கும்.**
- "ராஜா - ஆண் + பெண் = ராணி" என்று கணித ரீதியாக காட்டலாம்:
  - `vector("ராஜா") - vector("ஆண்") + vector("பெண்") ≈ vector("ராணி")`

---

### **Word2Vec-ன் இரண்டு முக்கிய முறைகள் :**

#### **1. Continuous Bag of Words (CBOW):**
- இந்த முறையில், சுற்றியுள்ள வார்த்தைகள் (context words) கொடுக்கப்பட்டு, இலக்கு வார்த்தை (target word) கணிக்கப்படுகிறது.
- எடுத்துக்காட்டு: "The cat sat on the ___" என்ற வாக்கியத்தில், "___" இடத்தில் "mat" என்ற வார்த்தையை கணிக்க CBOW பயன்படுத்தப்படுகிறது.
- **CBOW-ன் நன்மைகள்:**
  - சிறிய தரவு மூலம் நன்றாக வேலை செய்யும்.
  - வேகமானது மற்றும் குறைந்த கணினி வளங்கள் தேவை.

#### **2. Skip-Gram:**
- இந்த முறையில், இலக்கு வார்த்தை (target word) கொடுக்கப்பட்டு, சுற்றியுள்ள வார்த்தைகள் (context words) கணிக்கப்படுகின்றன.
- எடுத்துக்காட்டு: "cat" என்ற வார்த்தை கொடுக்கப்பட்டு, "The", "sat", "on", "the" போன்ற சுற்றியுள்ள வார்த்தைகள் கணிக்கப்படுகின்றன.
- **Skip-Gram-ன் நன்மைகள்:**
  - பெரிய தரவு மூலம் நன்றாக வேலை செய்யும்.
  - அரிதான வார்த்தைகளுக்கு (rare words) நன்றாக வேலை செய்யும்.

Word2Vec-ன் பயன்பாடுகள்:

1. **தேடுபொறிகள் (Search Engines):**
   - தேடல் முடிவுகளின் துல்லியத்தை மேம்படுத்த Word2Vec பயன்படுத்தப்படுகிறது.
   - எடுத்துக்காட்டு: "apple" என்ற வார்த்தை பழத்தை குறிக்கிறதா அல்லது நிறுவனத்தை குறிக்கிறதா என்பதை Word2Vec புரிந்து கொள்ளும்.

2. **மொழிபெயர்ப்பு (Language Translation):**
   - Google Translate போன்ற மொழிபெயர்ப்பு பயன்பாடுகளில் Word2Vec பயன்படுத்தப்படுகிறது.

3. **வாடிக்கையாளர் கருத்து பகுப்பாய்வு (Customer Feedback Analysis):**
   - வாடிக்கையாளர் கருத்துகளை பகுப்பாய்வு செய்ய Word2Vec பயன்படுத்தப்படுகிறது.

4. **பரிந்துரை அமைப்புகள் (Recommendation Systems):**
   - பயனர்களின் தேடல் வரலாறு மற்றும் வாங்கிய பொருட்களை அடிப்படையாக கொண்டு பரிந்துரை அமைப்புகள் உருவாக்கப்படுகின்றன.

Word2Vec-ன் குறைபாடுகள்:

1. **அறியப்படாத வார்த்தைகளை கையாளும் திறன் இல்லை:**
   - Word2Vec அறியப்படாத வார்த்தைகளை கையாள முடியாது.

2. **உப-வார்த்தை நிலைகளில் பகிரப்பட்ட பிரதிநிதித்துவங்கள் இல்லை:**
   - Word2Vec ஒவ்வொரு வார்த்தையையும் ஒரு தனி வெக்டராக பிரதிநிதித்துவப்படுத்துகிறது.

3. **புதிய மொழிகளுக்கு அளவிடுவது கடினம்:**
   - புதிய மொழிகளுக்கு Word2Vec-ஐ அளவிடுவது கடினம்.

குறியீடு மூலம் புரிதல் :

**Python-ல் Word2Vec மாதிரியை பயிற்சி செய்தல்:**

```python
from gensim.models import Word2Vec

# உதாரண வாக்கியங்கள்
sentences = [
    ["I", "love", "machine", "learning"],
    ["AI", "is", "fascinating"],
    ["I", "study", "NLP"]
]

# Word2Vec மாதிரியை பயிற்சி செய்தல்
model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)

# "king" என்ற வார்த்தையின் embedding-ஐ பெறுதல்
king_vector = model.wv['king']
print("ராஜா-ன் embedding:", king_vector)

# "king" என்ற வார்த்தைக்கு ஒத்த வார்த்தைகளை கண்டறிதல்
similar_words = model.wv.most_similar('king')
print("ராஜா-க்கு ஒத்த வார்த்தைகள்:", similar_words)
```

Gensim-ஐப் பயன்படுத்தி **Word2Vec** மாடல் ஒவ்வொரு வார்த்தைக்கும் ஒரு எம்பெடிங் (vector representation) உருவாக்குகிறது. மாடல் இந்த எம்பெடிங்களைக் கொண்டு வார்த்தைகளுக்கு இடையிலான தொடர்பை (semantic similarity) கண்டறிகிறது.

model.wv.most_similar('king') என்ற அழைப்பின் மூலம், **“king”** என்ற வார்த்தைக்கு ஆக்க பக்கத்தில் உள்ள தொடர்புடைய வார்த்தைகளை (similar words) கண்டறிகிறது.

```python
 [('ruler', 0.25290292501449585), ('queen', 0.1703130453824997), 
 ('are', 0.15026721358299255), 
 ('fascinating', 0.13887712359428406), 
 ('NLP', 0.10853718221187592), 
 ('love', 0.03476576507091522), 
 ('and', 0.01612497679889202), 
 ('I', 0.00450251717120409), 
 ('study', -0.005892974324524403), 
 ('is', -0.02770209312438965)]
```

('ruler', 0.2529): “ruler” என்பது **king**-க்கு மிக அண்மையான பொருள் கொண்ட வார்த்தை என மாடல் கருதுகிறது. இந்த similarity score **0.2529** என்பது **king** மற்றும் **ruler** வார்த்தைகளின் வெக்டர் இடையிலான cosine similarity அடிப்படையில் கணக்கிடப்பட்டது.

('queen', 0.1703): “queen” என்பது **king**-க்கான தொடர்புடைய மற்றொரு வார்த்தையாக இருக்கிறது. இது “ruler”-க்கு அடுத்தடுத்த பொருளுடையது, ஆனால் குறைவான ஒத்தபாடுடையது (**0.1703**).

('are', 0.1503): இது எம்பெடிங் டேட்டாவிலிருந்து, **are** என்ற வார்த்தை “king”-க்குப் பக்கத்தில் தோன்றியதால் ஒத்தவாறு தெரிகிறது.

('fascinating', 0.1389): இதுவும் **king**-க்கு தொடர்புடையதாக கண்டறியப்பட்டது, ஆனால் semantic தொடர்பு (பொருள் தொடர்பு) இல்லாமல், கற்றல் தரவின் அடிப்படையில் மட்டும் தோன்றுகிறது.

('NLP', 0.1085): **NLP** போன்ற வார்த்தை காரணமாக, மாதிரி மிகச்சிறிய data-வில் இருந்து இதுபோன்ற தவறான தொடர்புகளை வெளிப்படுத்துகிறது.

('love', 0.0347) மற்றும் பிற வார்த்தைகள்: இது மிகவும் குறைந்த ஒத்தபாடுடையது, எனவே **king**-க்கு இது பொருத்தமற்றது எனவும் கூறலாம்.

இங்கே பயிற்சி செய்த தரவுத்தொகுப்பு (sentences) மிகச் சிறியது, மற்றும் “king” வார்த்தையை சரியான “context”-இல் அதிகமில்லை. ஆகவே, பல வார்த்தைகள் அவற்றின் உண்மையான semantic தொடர்புகளைச் சரியாக பிரதிபலிக்க முடியவில்லை.

**தீர்வு**: பெரிய மற்றும் பலதரப்பட்ட dataset-ல் மாடலை train செய்ய வேண்டும். “king” போன்ற வார்த்தைகள் ஏற்ற semantic context-இல் (e.g., royalty, leadership) அடிக்கடி தோன்ற வேண்டும்.

Word2Vec என்பது உரை தரவை கணினிகள் புரிந்து கொள்ளும் வகையில் எண்களாக மாற்றும் ஒரு சக்திவாய்ந்த கருவி. இது NLP-ல் மிகவும் முக்கியமானது மற்றும் மெஷின் கற்றல் மாதிரிகளின் துல்லியத்தை மேம்படுத்துகிறது. Word2Vec-ன் இரண்டு முறைகள் (CBOW மற்றும் Skip-Gram) மூலம், வார்த்தைகளின் அர்த்தம் மற்றும் தொடர்புகளை பிரதிநிதித்துவப்படுத்த முடியும்.


