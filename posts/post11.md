# Embedding

**Embedding** என்பது, **ஒவ்வொரு Token-ஐயும் (சொல்லை) எண்ணியல் வெக்டராக மாற்றும் செயல்முறை**. இந்த வெக்டர்கள் **Neural Network**-களால் புரிந்து கொள்ளக்கூடிய வடிவத்தில் இருக்கும். இது LLM-களுக்கு **சொற்களுக்கு இடையே உள்ள தொடர்புகளைக் கற்றுக்கொள்ள** உதவுகிறது.

## Embedding-கள் என்றால் என்ன?

Embedding-கள் என்பது ஒரு வகையான **தரவு பிரதிநிதித்துவம் (Data Representation)**. இது உரை (text), படங்கள் (images), ஒலி (audio) போன்ற தரவுகளை எண்களாக மாற்றி, கணினிகள் புரிந்து கொள்ளும் வகையில் காட்டுகிறது. Embedding-கள் முக்கியமாக **Natural Language Processing (NLP)**-ல் பயன்படுத்தப்படுகிறது, அங்கு வார்த்தைகள், வாக்கியங்கள் அல்லது பத்திகள் எண்களாக மாற்றப்படுகின்றன.

## எப்படி Embedding-கள் செயல்படுகின்றன?

### 1. **Token to Vector:**  
   - ஒவ்வொரு Token-ஐயும் (சொல்லை) ஒரு **தனிப்பட்ட எண்ணியல் வெக்டராக** மாற்றுகிறது.  
   - **உதாரணம்:**  
     - **Token:** "Programming"  
     - **Embedding:** [0.25, -0.12, 0.87, ..., 0.45] (ஒரு நீண்ட எண் வரிசை).  
   - **விளக்கம்:**  
     - "Programming" என்ற சொல் ஒரு எண் வரிசையாக (Vector) மாற்றப்படுகிறது. இந்த வெக்டர், அந்த சொல்லின் **பொருள் மற்றும் பண்புகளை** பிரதிபலிக்கிறது.

### 2. **Vector Dimensions:**  
   - இந்த வெக்டர்கள் பொதுவாக **100 முதல் 1000 வரையிலான பரிமாணங்களைக் கொண்டிருக்கும்**.  
   - **உதாரணம்:**  
     - GPT-3 மாடல்களில், ஒவ்வொரு Token-க்கும் **768 பரிமாணங்கள்** உள்ளன.  
   - **விளக்கம்:**  
     - ஒவ்வொரு சொல்லும் 768 எண்களைக் கொண்ட ஒரு வெக்டராக மாற்றப்படுகிறது. இது அந்த சொல்லின் **சிக்கலான பண்புகளை** பிரதிபலிக்கிறது.

### 3. **Semantic Meaning:**  
   - Embedding-கள் **சொற்களின் பொருளைப் பிரதிபலிக்கின்றன**.  
   - **உதாரணம்:**  
     - "King" மற்றும் "Queen" என்ற சொற்களின் Embedding-கள் ஒரே மாதிரியாக இருக்கும், ஆனால் அவை வெவ்வேறு பாலினங்களைக் குறிக்கின்றன.  
   - **விளக்கம்:**  
     - "King" மற்றும் "Queen" என்ற சொற்கள் ஒரே மாதிரியான Embedding-களைக் கொண்டிருக்கும், ஏனெனில் அவை இரண்டும் **அரசர்களை** குறிக்கின்றன. ஆனால், அவை வெவ்வேறு பாலினங்களைக் குறிப்பதால், அவற்றின் Embedding-கள் சிறிது வேறுபடும்.

### 4. **Neural Network Input:**  
   - இந்த Embedding-கள் **Neural Network**-க்கு உள்ளீடாக அனுப்பப்படுகின்றன.  
   - **உதாரணம்:**  
     - "Python is a programming language."  
     - **Tokenization:** ["Python", "is", "a", "programming", "language"].  
     - **Embedding:**  
       - "Python" → [0.12, -0.45, 0.67, ..., 0.89]  
       - "programming" → [0.25, -0.12, 0.87, ..., 0.45]  
       - "language" → [0.34, -0.56, 0.78, ..., 0.23]  
   - **விளக்கம்:**  
     - ஒவ்வொரு சொல்லும் ஒரு எண் வரிசையாக மாற்றப்பட்டு, Neural Network-க்கு உள்ளீடாக அனுப்பப்படுகிறது. இது மாடலை **சொற்களுக்கு இடையே உள்ள தொடர்புகளைக் கற்றுக்கொள்ள** உதவுகிறது.

---

## Embedding-களின் முக்கியத்துவம்

### 1. **சொற்களின் பொருள்:**  

   - Embedding-கள் சொற்களின் பொருளைப் பிரதிபலிக்கின்றன.  
   - **உதாரணம்:**  
     - "King" மற்றும் "Ruler" என்ற சொற்களின் Embedding-கள் ஒரே மாதிரியாக இருக்கும், ஏனெனில் அவை ஒரே பொருளைக் கொண்டவை.  
   - **விளக்கம்:**  
     - "King" மற்றும் "Ruler" என்ற சொற்கள் ஒரே பொருளைக் கொண்டிருப்பதால், அவற்றின் Embedding-கள் ஒரே மாதிரியாக இருக்கும்.

### 2. **சொற்களுக்கு இடையே உள்ள தொடர்புகள்:**  

   - Embedding-கள் சொற்களுக்கு இடையே உள்ள தொடர்புகளைக் காட்டுகின்றன.  
   - **உதாரணம்:**  
     - "King" - "Man" + "Woman" = "Queen".  
   - **விளக்கம்:**  
     - இந்த கணித செயல்பாடு, "King" என்ற சொல்லின் Embedding-லிருந்து "Man" என்ற சொல்லின் Embedding-ஐ கழித்து, "Woman" என்ற சொல்லின் Embedding-ஐ கூட்டினால், "Queen" என்ற சொல்லின் Embedding கிடைக்கும். இது Embedding-கள் சொற்களுக்கு இடையே உள்ள தொடர்புகளைப் புரிந்து கொள்வதைக் காட்டுகிறது.

### 3. **Neural Network-க்கு உள்ளீடு:**  

   - Embedding-கள் Neural Network-க்கு உள்ளீடாக அனுப்பப்படுகின்றன, இது மாடலை **சொற்களுக்கு இடையே உள்ள தொடர்புகளைக் கற்றுக்கொள்ள** உதவுகிறது.  

   - **உதாரணம்:**  
     - "Python is a programming language."  
     
   - **விளக்கம்:**  

     - இந்த வாக்கியத்தில் உள்ள ஒவ்வொரு சொல்லும் Embedding-ஆக மாற்றப்பட்டு, Neural Network-க்கு உள்ளீடாக அனுப்பப்படுகிறது. இது மாடலை **வாக்கியத்தின் அர்த்தத்தைப் புரிந்து கொள்ள** உதவுகிறது.

Embedding-கள் என்பது உரை தரவை கணினிகள் புரிந்து கொள்ளும் வகையில் எண்களாக மாற்றும் ஒரு சக்திவாய்ந்த கருவி. இது NLP-ல் மிகவும் முக்கியமானது மற்றும் மெஷின் கற்றல் மாதிரிகளின் துல்லியத்தை மேம்படுத்துகிறது. Word2Vec, GloVe, மற்றும் BERT போன்ற முன்-பயிற்சி பெற்ற embeddings நிறைய பணிகளுக்கு பயன்படுத்தப்படுகின்றன. இவை மொழி மாதிரிகளின் செயல்திறனை மேம்படுத்துவதில் முக்கிய பங்கு வகிக்கின்றன.

